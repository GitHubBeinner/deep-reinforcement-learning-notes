# Macro-action SL

## 介绍

> [Learning Macromanagement in StarCraft from Replays using Deep Learning](https://arxiv.org/abs/1707.03743)

实时战略游戏星际争霸已被证明是一个充满挑战的人工智能技术环境，因此，目前最先进的解决方案包括许多手工制作的模块。在本文中，我们将展示如何使用深度学习直接从游戏回放中学习星际争霸中的宏观管理决策。神经网络训练了789,571个状态 - 动作对，从2,005个高手玩家的重放中提取，在预测下一个构建动作时达到54.6％和22.9％的前1和前3错误率。通过将训练有素的网络整合到开源星际争霸机器人——人工智能机器人\(UAlbertaBot\)中，该系统可以显著优于游戏内置的人族机器人，并以固定的冲刺策略与人工智能机器人\(UAlbertaBot\)进行竞争。据我们所知，这是第一次直接从星际争霸的重播中学习宏观管理任务。虽然最好的手工策略仍是最先进的，但深度网络方法能够表达多种不同的策略，从而通过深度强化学习进一步提高网络性能，这是未来研究的一个很有前途的途径。最终，这种方法可能会导致强大的星际争霸机器人对硬编码策略的依赖性降低。

## 方法

### Dataset

重放文件是二进制格式，需要预处理才能提取知识。本文中使用的数据集是从现有数据集中提取的。辛纳伊夫等人。通过清理三个星际争霸社区网站GosuGamers、ICCupand TeamLiquid，收集了7649次回放，这些网站主要是为包括专业人士\[27\]在内的高技能玩家提供的。罗伯逊等人从存储库中提取了大量信息并存储在一个数据库中。\[22\]。这个数据库包含游戏中每24帧的状态变化，包括单位属性。我们的数据集是从这个数据库中提取的，预处理步骤的另一个视图如图所示：

![](../../.gitbook/assets/image%20%28126%29.png)

### Network Architecture

![](../../.gitbook/assets/image%20%2876%29.png)

### Training

789，571个状态-动作对的数据集被分成631，657对\(80%\)的训练集和157，914对\(20%\)的测试集。训练集专门用于训练网络，而测试集用于评估训练网络。状态-动作对来自2005个不同的神族和人族游戏，在分割数据之前不会被打乱，以避免来自同一个游戏的动作同时出现在训练和测试集中

### Applying the Network to a StarCraft Bot

学习预测人类玩游戏中的行为与学习游戏的行为非常相似。 然而，这种类型的模仿学习确实有其局限性，因为代理人不会学会采取最佳行动，而是采取最可能的行动（如果人类正在玩）。 然而，将训练有素的网络应用为现有机器人的宏观管理模块可能是朝着更先进的方法迈出的重要一步。

在本文中，我们构建了UAlbertaBot，它有一个生产管理器，管理机器人必须按顺序生成的构建队列。 通常使用基于目标的搜索的生产经理被修改为使用在重放上训练的网络。 UAlbertaBot的制作经理也被扩展为网络客户端; 当要求下一个构建时，请求被转发到网络服务器以及当前游戏状态的描述，该网络服务器将游戏状态馈送到神经网络，然后返回到模块的构建预测。 由于网络只接受过Protoss与Terran游戏的训练，因此只能在这场比赛中进行测试。

## 实验

![](../../.gitbook/assets/image%20%2863%29.png)

![](../../.gitbook/assets/image%20%281%29.png)

可以看到经过精心手工策略设计的UAlbertaBot仍然是效果最好的，但是从回放中学习仍为星际争霸智能提供了一个新方向。





