# IQL

## 介绍

> [Stabilising experience replay for deep multi-agent reinforcement learning](https://arxiv.org/abs/1702.08887)

许多现实问题，例如网络包路由和城市交通控制，都被自然地建模为多代理执行学习（RL）问题。然而，现有的多代理RL方法通常在问题规模上难以扩展。因此，一个关键的挑战是将单智能体学习的成功转化为多智能体环境。个主要的障碍是独立的Q学习，这是最流行的多智能体RL方法，它引入了非平稳性，这使得它与深度Q学习依赖的体验重放记忆不兼容。本文提出了解决这一问题的两种方法：1）使用重要抽样的多代理变体自然地衰减过时数据;2\)在一个指纹上调整每个代理的值函数，该指纹可以消除重放存储器中数据样本的年龄歧义。一个挑战分散的星际争霸单位微观管理变体的结果证实，这些方法能够成功地将经验重放与多智能体RL相结合。

## 方法

一种多代理学习方法是独立的Q学习（IQL）（Tan，1993），其中每个代理人独立地学习其策略，将其他代理人视为环境的一部分。 虽然IQL避免了中心学习的可扩展性问题，但它引入了一个新问题：从每个代理的角度来看，环境变得非平稳，因为它包含了其他自己学习的代理，排除了任何收敛保证。 幸运的是，大量的经验证据表明，IQL在实践中经常运作良好。

经验重放与IQL的结合似乎是有问题的：IQL引入的非平稳性意味着在代理的重放内存中生成数据的动态不再反映它正在学习的当前动态。

为了避免将IQL和经验重放结合起来的困难，以前对深度多代理RL的研究将经验重放的使用限制在短的、最近的缓冲区。，2017年\)或干脆完全禁用重播\(Foersteret al .，2016年\)。然而，这些变通办法限制了效率，并威胁到多代理RL的稳定性。在这一部分中，我们提出了两种有效地将经验重放融入多智能体学习的方法。

### Multi-Agent Importance Sampling

### Multi-Agent Fingerprints



