# IQL

## 介绍

> [Stabilising experience replay for deep multi-agent reinforcement learning](https://arxiv.org/abs/1702.08887)

许多现实问题，例如网络包路由和城市交通控制，都被自然地建模为多代理执行学习（RL）问题。然而，现有的多代理RL方法通常在问题规模上难以扩展。因此，一个关键的挑战是将单智能体学习的成功转化为多智能体环境。个主要的障碍是独立的Q学习，这是最流行的多智能体RL方法，它引入了非平稳性，这使得它与深度Q学习依赖的体验重放记忆不兼容。本文提出了解决这一问题的两种方法：1）使用重要抽样的多代理变体自然地衰减过时数据;2\)在一个指纹上调整每个代理的值函数，该指纹可以消除重放存储器中数据样本的年龄歧义。一个挑战分散的星际争霸单位微观管理变体的结果证实，这些方法能够成功地将经验重放与多智能体RL相结合。

## 方法

一种多代理学习方法是独立的Q学习（IQL）（Tan，1993），其中每个代理人独立地学习其策略，将其他代理人视为环境的一部分。 虽然IQL避免了中心学习的可扩展性问题，但它引入了一个新问题：从每个代理的角度来看，环境变得非平稳，因为它包含了其他自己学习的代理，排除了任何收敛保证。 幸运的是，大量的经验证据表明，IQL在实践中经常运作良好。

经验重放与IQL的结合似乎是有问题的：IQL引入的非平稳性意味着在代理的重放内存中生成数据的动态不再反映它正在学习的当前动态。

为了避免将IQL和经验重放结合起来的困难，以前对深度多代理RL的研究将经验重放的使用限制在短的、最近的缓冲区。，2017年\)或干脆完全禁用重播\(Foersteret al .，2016年\)。然而，这些变通办法限制了效率，并威胁到多代理RL的稳定性。在这一部分中，我们提出了两种有效地将经验重放融入多智能体学习的方法。

### Multi-Agent Importance Sampling

我们可以通过为多智能体设置开发重要性采样方案来解决IQL中存在的非平稳性问题。如RL代理可以使用重要性抽样来从其自身策略不同时收集的数据中学习策略，同样也可以从off-environment中收集的数据中学习环境（Ciosek＆Whiteson，2017）。由于IQL将其他代理的策略视为环境的一部分，因此可以使用非环境重要性采样来稳定体验重放。我们首先考虑一个完全可观察的多代理设置。 如果Q函数可以直接关注真实状态，我们可以根据所有其他代理的策略为单个代理编写Bellman optimality等式：

![](../../.gitbook/assets/image%20%28115%29.png)

其中 $$\pi_{-a}\left(\mathbf{u}_{-a} | s\right)=\Pi_{i \in-a \pi_{i}}\left(u_{i} | s\right)$$ ，重要性采样损失可表示为：

![](../../.gitbook/assets/image%20%2829%29.png)

下面讨论部分可观察的情况，定义可观察状态为 $$\hat{s}=\left\{s, \tau_{-a}\right\} \in \hat{S}=S \times T^{n-1}$$ ，即s与其它代理观察状态的并集。在此基础上，我们可以定义部分可观察的回报函数、状态转移方程：

![](../../.gitbook/assets/image%20%28167%29.png)

![](../../.gitbook/assets/image%20%286%29.png)

定义部分可观察的MDP $$\widehat{G}=\langle\hat{S}, U, \hat{P}, \hat{r}, Z, \hat{O}, n, \gamma\rangle$$ ，Q函数为：

![](../../.gitbook/assets/image%20%2825%29.png)

![](../../.gitbook/assets/image%20%2871%29.png)

然而，与完全可观察到的情况不同，右手边包含其他几个间接依赖于其他代理的策略的术语，据我们所知，这些术语是难以处理的。所以部分可观察情况下的重要性采样系数为近似值。

### Multi-Agent Fingerprints

虽然重要性抽样提供了对真实目标的无偏估计，但它通常产生具有大和无界方差的重要性比率（Robert＆Casella，2004）。调整或调整重要性权重可以减少方差但引入偏差。 因此，我们提出了一种替代方法，它包含多代理问题的非平稳性，而不是纠正它。

IQL的弱点在于，通过将其他代理视为环境的一部分，它忽略了这样一个事实，即这些代理的策略会随着时间的推移而改变，从而使其自身的q功能变得不稳定。这意味着，如果Q函数以其他代理的策略为条件，它就可以变成静止的。这正是hyper Q-learning背后的哲学\(Tesauro，2003\):通过贝叶斯推理计算出的对其他智能体策略的估计，每个智能体的状态空间得到了增强。直观地说，这将每个代理的学习问题简化为一个标准的、单一代理的问题，在一个静止但更大的环境中。

hyper Q-learning的实际困难在于它增加了Q函数的维数，使其学习成为可能的不可行。考虑用神经网络的参数表示其它代理策略，其数量将会非常庞大。

然而，一个关键的观察结果是，为了稳定经验重放，每个代理不需要建立在任何任何可能的 $$\boldsymbol{\theta}_{-a}$$ 上，但实际上只有那些 $$\boldsymbol{\theta}_{-a}$$ 的值出现在其重放存储器中。在这个缓冲区中生成数据的策略序列可以被认为是在高维策略空间中遵循单一的一维轨迹。为了稳定经验重现，如果每个代理人的观察结果都明确了当前训练样本沿该轨迹的来源，这就足够了。

那么，他的问题是如何设计一个包含这些信息的低维fingerprint。显然，给定其他代理的策略，这种指纹必须与状态-动作对的真实值相关联。它应该在培训过程中平稳地变化，以允许模型在其他代理在学习过程中执行不同质量的策略的经验中具体化。指纹中一个明显的候选是训练操作编号。一个潜在的挑战是，在策略收敛后，这需要模型将多个指纹匹配到同一个值，从而使函数变得更难学习，也更难概括。

其他代理人表现的另一个关键因素是探索率？ 通常，退火计划设定为？使得它在整个训练过程中平稳地变化并且与性能密切相关。 因此，我们用？进一步增加Q函数的输入，使得观察函数变为 $$\{O(s), \epsilon, e\}$$ 。

## 实验

![](../../.gitbook/assets/image%20%2879%29.png)





