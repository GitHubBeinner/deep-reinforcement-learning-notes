# Playing Atari with Six Neurons

## 介绍

> [Playing Atari with Six Neurons](https://arxiv.org/pdf/1806.01363.pdf)

深度强化学习，应用于像Atari游戏这样的基于视觉的问题，将像素直接映射到动作; 在内部，深度神经网络负责提取有用的信息并基于它做出决策。 通过将图像处理与决策分开，可以更好地理解每个任务的复杂性，并且可能找到更容易让人理解并且可以更好地概括的更小的策略表示。 为此，我们提出了一种新的方法来分别学习策略和紧凑状态表示，但同时用于强化学习中的策略近似。 状态表示由编码器基于两种新算法生成：Increase Dictionary Vector Quantization使编码器能够随着时间的推移增加其字典大小，以便在开放式在线学习环境中出现时发现新的观察状态; Direct Residuals Sparse Coding通过忽略重建误差最小化来编码输入状态。 编码器在线自主选择观察以进行训练，以便最大化编码稀疏性。 随着字典大小的增加，编码器为神经网络产生越来越大的输入：这通过Exponential Natural Evolution Strategies的变体来解决，该算法适应沿着运行的概率分布维度。 我们使用仅有6到18个神经元的神经网络（取决于游戏的控制），在选择的Atari游戏上测试我们的系统。 这些技术仍然能够取得与使用多两个数量级神经元的最先进技术相当——有时甚至更好——的结果。

## 算法

![](../../.gitbook/assets/image%20%2810%29.png)

我们的系统分为四个主要部分: 1 \)Environment是雅达利游戏\(ALE\)，采取行动并提供观察状态；2\)Compressor从观察中提取低维代码，同时与系统的其余部分一起在线训练；3\)Controller是我们的策略近似器，即神经网络；4\)最后，Optimizer是我们的学习方法，随着时间的推移提高网络的性能，在我们的例子中是一种进化策略。下面将更详细地描述每个组件。

### Compressor

Compressor的作用是为来自环境的每一个状态提供一个压缩的特征表达，使神经网络完全集中于决策。这是通过无监督学习来实现的，通过网络与环境的互动，以在线学习的方式获得相同的观察结果。

#### Vanilla vector quantization

标准VQ算法是一种基于字典的编码技术，应用于降维和压缩。 空间中的代表性元素（称为质心，统称为字典）以类似于k-means的方式充当周围空间的参考。空间中元素的编码是一个向量，其中每个位置对应于一个质心。  它的值通常上设置为零，除了与空间中最接近的代表质心相对应的位置。使用密集的编码向量，捕获多个质心以获得更高的精度。 在任何一种情况下，原数据可以被压缩为编码和字典之间的向量。 其重建恢复的数据与原数据之间的差异称为重构错误，并且量化了压缩/解压缩过程中丢失的信息。 通过在训练集上调整质心最小化重建误差来训练字典。

#### Increasing Dictionary VQ

![](../../.gitbook/assets/image%20%2821%29.png)

为了使VQ算法能在RL环境中应用，我们引入了增加字典向量量化\( IDVQ\)，这是一种基于向量量化的新压缩器，它在连续的训练迭代中自动增加字典的大小，特别为在线学习而定制。IDVQ没有固定大小的字典，而是从一个简单的字典开始，因此不需要初始化，并随着学习的进展添加新的质心。

这是通过从重建误差的正部分建立新的质心来实现的，该质心对应于原始图像的信息\(在0和1之间重新缩放\)，而原始图像不是通过当前编码重建的\(参见算法1 \)。字典大小的增长受阈值 $$δ$$ 的调节，这表明最小聚集残差被认为是有意义的加法。训练集是通过统一采样一代中所有个体获得的观察值而建立的。

添加到字典中的质心没有进一步细化。这符合图像区分的目标，而不是最小化重建误差:每个质心都被故意构造成表示一个特定的特征，这是在实际观测中发现的，以前在字典中没有。

然而，增加字典大小会改变编码大小，进而改变神经网络的输入大小。这要求控制器和优化器都要小心更新，这将在后面描述。

#### Direct Residuals Sparse Coding

![](../../.gitbook/assets/image%20%2865%29.png)

基于字典方法的算法的性能更多地取决于编码的选择而不是字典训练 - 在使用训练有素的质心与随机选择的样本时，性能最佳的算法只有很小的性能提升。 这突出了选择有效编码算法以最好地利用IDVQ训练的字典特征的重要性。 近年来，一些研究显示基于稀疏编码的算法在压缩和重建任务中始终如一地表现最佳。这些通常交替训练质心并最小化编码的1规范（近似于0范数），最终产生代码这主要由零组成。 在我们的案例中，字典已经使用IDVQ训练：我们更专注于稀疏编码的构造。

构造稀疏编码的经典方法是通过迭代方法，其中在每个步骤\( i \)选择很少的质心，\( ii \)构建相应的编码，以及\( iii \)基于重构误差评估代码质量，以“1范数”作为正则项。该过程在不同的质心组合上重复进行，以增量累加的方式减少重建误差，但代价是算法的性能。 此外，重建是作为代码和字典之间的矢量积来计算的：虽然在概念上很优雅，但这个点积产生一个线性组合（质心和编码值），其中大多数项具有零系数。

在我们的情况中，重点是区分状态以支持决策者，而不是最优化原始输入的重新构造。编码算法将依赖于来自环境的每一个观察，相应地减少了可用于决策的计算时间。这迫使编码器的目标功能从根本上彻底变革，优先考虑决策区分，即在重构误差上观察和区分。

为此，我们引入了直接残差稀疏编码（DRSC，算法2）作为一种新颖的稀疏编码算法，专门用于在最短的时间内产生高度差异化的编码。 其主要特点是：（i）利用质心构造为来自IDVQ的residual images，从而避免了质心训练阶段; （ii）它产生二进制编码，将重建过程简化为对应于码的非零系数的质心上的无加权和; （iii）它在单个遍历中生成代码，在选择少量质心后提前终止。 结果是一个具有线性性能的超理想尺寸的算法，它将一个观察分解成字典中发现的最相似的成分。

#### step-by-step breakdown

该算法一直循环并添加质心，直到（聚合的）残差信息低于阈值，对应于捕获原始图像中的信息的任意精度。 为了在字典中没有正确的质心的情况下强制实施稀疏性，编码循环的第二个停止标准是当基于另一个阈值时将多个质心添加到代码中时。 编码后具有高残差信息的图像是压缩机训练的主要候选者。

字典使用IDVQ进行训练添加新的质点，以最小化编码中的剩余残差信息。训练开始于从训练集中选择一个图像并将用DRSC编码，产生二进制编码。编码和字典之间的点积\(即对代码选择的质心求和，因为它是二进制的\)产生原始图像的重建，类似于基于字典的算法。

训练图像和重建之间的差异然后产生重建误差（-image），其中值的符号再次对应于它们的原点：正值是来自图像的剩余信息，其在重建中未被编码， 而负值是与原始图像无关的重构伪像。 然后聚合该重构错误图像（带有和）以估计编码遗漏的信息量。 如果高于给定阈值，则应在字典中添加新的质心，以使DRSC能够进行更精确的重构。但在这种情况下，残差本身会产生完美的质心，因为它会精确捕获当前编码所遗漏的信息，然后被添加到字典中。

### Controller

























