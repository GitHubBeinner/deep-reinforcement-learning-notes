# ES

## 介绍

> [Evolution Strategies as a Scalable Alternative to Reinforcement Learning](https://arxiv.org/pdf/1703.03864.pdf)

我们探索使用Evolution Strategies（ES），一类黑盒优化算法，作为基于MDP的流行RL技术的替代方案，如Q-learning和Policy Gradients。 MuJoCo和Atari上的实验表明，ES是一种可行的解决方案策略，可以根据可用的CPU数量进行非常好的扩展：通过使用基于常见随机数的新型通信策略，我们的ES实现只需要传达标量，使其可以扩展到超过一千个并行进程。 这使我们能够在10分钟内解决3D人体行走，并在训练一小时后获得大多数Atari游戏的竞争结果。 此外，我们强调了作为黑盒优化技术的ES的几个优点：它不受行动频率和延迟奖励的影响，能够容忍极长的视野，并且不需要temporal discounting或值函数逼近。

本文的主要发现如下：

1. 我们发现使用虚拟批量标准化极大地提高了进化策略的可靠性。 如果没有这些方法，ES在我们的实验中证明是脆弱的，但通过这些重新参数化，我们在各种环境中取得了很好的结果。
2. 我们发现进化策略方法可高度并行化：通过引入基于常见随机数的新型通信策略，即使使用超过一千名工作人员，我们也能够在运行时实现线性速度。 特别是，使用1,440个工作进程，我们能够在10分钟内解决MuJoCo 3D人形任务。
3. 进化策略的数据效率惊人地好:我们能够匹配A3C 在大多数Atari环境中，使用的数据量在3倍到10倍之间。由于没有执行反向传播和没有值函数，所需的计算量减少了大约3倍，这在一定程度上抵消了数据效率的轻微下降。ES1小时训练结果需要的计算量与A3C1天训练结果相同，而在23个游戏测试上表现更好，在28个上表现更差。在MuJoCo任务中，我们能够匹配信任区域策略优化，使用的数据最多不超过10倍。
4. 我们发现ES表现出比像TRPO这样的政策梯度方法更好的探索行为:在MuJoCo人形机器人任务中，ES已经能够学习各种各样的步态\(比如侧向行走或者向后行走\)。TRPO从未观察到这些不寻常的步态，这表明了一种本质上不同的勘探行为
5. 我们发现进化策略方法是稳健的:对于所有Atari环境，我们使用固定超参数实现了上述结果，对于所有MuJoCo环境，我们使用了一组不同的固定超参数\(一个二元超参数除外，它在不同的MuJoCo环境中并不保持恒定\)

## 算法

### 进化策略

进化算法的常规流程为：初始种群&gt;&gt;交叉变异&gt;&gt;评价&gt;&gt;筛选下一代......直到问题收敛

令 $$F$$ 为评价函数， $$\theta$$ 为策略网络参数，扰动策略分布 $$p_{\psi}(\theta)$$ 为种群

则优化的最终目标是最大化 $$\mathbb{E}_{\theta \sim p_{\psi}} F(\theta)$$ ，所以得到梯度

$$\nabla_{\psi} \mathbb{E}_{\theta \sim p_{\psi}} F(\theta)=\mathbb{E}_{\theta \sim p_{\psi}}\left\{F(\theta) \nabla_{\psi} \log p_{\psi}(\theta)\right\}$$ 

令扰动分布服从高斯分布，则有

$$\nabla_{\theta} \mathbb{E}_{c \sim N(0, I)} F(\theta+\sigma \epsilon)=\frac{1}{\sigma} \mathbb{E}_{c \sim N(0, I)}\{F(\theta+\sigma \epsilon) \epsilon\}$$ 

得到进化策略算法

![](../../.gitbook/assets/image%20%2836%29.png)

这里初始种群为带扰动的初始策略网络，评价函数为环境的回报，基于评价函数用梯度下降产生下一代群体，同时该高斯扰动可以作为强化学习中的探索策略。

### 进化策略的扩展和并行化

因为上述算法不需要评估值函数，而是直接用评估值反向传播训练策略网络，所以分布式通信中只需要传递评估值，而不需要样本轨迹，故通信开销极小。因此该算法可以有效的扩展到上千个工作进程。

![](../../.gitbook/assets/image%20%289%29.png)

### 参数空间中的平滑与动作空间中的平滑



