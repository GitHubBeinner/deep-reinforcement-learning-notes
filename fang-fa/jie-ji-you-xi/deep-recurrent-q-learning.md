# Deep Recurrent  Q-Network

## 介绍

> [Deep Recurrent Q-Learning for Partially Observable MDPs](https://arxiv.org/pdf/1507.06527.pdf)

深度强化学习为复杂任务提供了熟练的控制器。然而，这些控制器的内存有限，依赖于能够在每个决策点感知完整的游戏屏幕。为了解决这些缺点，本文研究了在深度Q-Network \(DQN\)中使用循环LSTM替换卷积后的第一个全连接层，从而增加递归性的效果。得到的深度递归Q-Network \(DRQN\)虽然在每个时间步上只能看到一个帧，但它成功地通过时间整合了信息，并复制了DQN在标准雅达利\(Atari\)游戏和部分观察到的具有闪烁游戏屏幕的等价物上的表现。此外，当使用部分观察进行训练并使用更完整的增量观察进行评估时，DRQN的性能作为可观察性的函数进行衡量。相反，当使用完整的观察进行训练并使用部分观察进行评估时，DRQN的性能下降小于DQN。因此,鉴于历史的长度相同,出现重复是一个可行的选择在DQN叠加帧的历史出现重复的输入层,并没有系统的优势学习玩游戏时,周期性网络可以更好的适应在评估时如果观测的质量变化。

## 算法

![](../../.gitbook/assets/image%20%2836%29.png)

