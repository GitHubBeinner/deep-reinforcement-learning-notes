# UNREAL

## 介绍

> [Reinforcement Learning with Unsupervised Auxiliary Tasks](https://arxiv.org/pdf/1611.05397)

深度强化学习代理通过直接最大化累积奖励获得了最先进的结果。然而，环境包含了更广泛的各种可能的训练信号。在这篇文章中，我们介绍了一个代理，它能同时通过强化学习最大化许多其他伪奖励函数。所有这些任务都有一个共同的表现形式，就像无监督学习一样，在没有外部奖励的情况下继续发展。我们还引入了一种新的机制，将这种表现集中在外在奖励上，这样学习就可以快速适应与实际任务最相关的方面。我们的代理在Atari上的表现明显优于之前的先进水平，平均880 %的专家人工表现，以及一系列具有挑战性的第一人称三维迷宫任务，速度提高10倍，平均达到87 %的人工表现。

## 算法

![](../../.gitbook/assets/image%20%2831%29.png)

### AUXILIARY CONTROL TASKS

给定一组辅助控制任务 $$C$$ ， $$π_(c)$$ 是每个辅助任务的代理策略 $$c \in \mathcal{C}$$ ， $$π$$ 是代理对基础任务的策略。 总体目标是在所有这些辅助任务中最大化总体性能。

$$\underset{\theta}{\arg \max } \mathbb{E}_{\pi}\left[R_{1 : \infty}\right]+\lambda_{c} \sum_{c \in C} \mathbb{E}_{\pi_{c}}\left[R_{1 : \infty}^{(c)}\right]$$ 

原则上为了同时学习多个辅助任务，应用off-policy，对于每个辅助任务都有一个单独的的损失函数

$$\mathbb{E}\left[\left(R_{t : t+n}+\gamma^{n} \max _{a^{\prime}} Q^{(c)}\left(s^{\prime}, a^{\prime}, \theta^{-}\right)-Q^{(c)}(s, a, \theta)\right)^{2}\right]$$

辅助回报函数如下：

* Pixel changes - 感知流的变化通常对应于环境中的重要事件。我们训练代理人学习一个单独的策略，最大限度地改变输入图像上的 $$n \times n$$ 非重叠网格的每个单元中的像素。
* Network features - 因为代理的策略或价值网络学会提取环境中与任务相关的高级特征。它们可能是代理学习控制的有用量。因此，激活代理神经网络的任何隐藏单元本身都可以是一种辅助奖励。我们训练代理，学习一个单独的策略，最大限度地激活特定隐藏层中的每个单元。

整体架构如图一所示

### AUXILIARY REWARD TASKS

在许多有趣的环境中，奖励非常稀疏，这意味着可能需要很长时间来训练擅长识别表示奖励开始的状态的特征提取器。 我们希望消除奖励和奖励状态的感性稀疏性，以帮助代理人的培训，但这样做的方式不会给代理人的策略带来偏差。

我们引入了前向预测的辅助任务 - reward prediction，在一些历史上下文下预测中间奖励。 该任务包括处理一系列连续观察，并要求代理预测在随后的帧中获得的奖励。 这类似于立即奖励（γ= 0）的价值学习。

### EXPERIENCE REPLAY

除了奖励预测之外，我们还使用重放缓冲器来执行value function replay。这相当于从行为策略分布重新采样最近的历史序列并且除了在策略值函数回归A3C之外执行额外值函数回归。 通过重新采样先前的经验，并随机改变计算n步返回的截断窗口的时间位置，值函数重放执行估值并利用由奖励预测形成的新发现的特征。 我们不会歪曲这种情况的分布

经验回放也用于提高辅助控制任务的效率和稳定性。Q学习更新被应用于从重放缓冲器中抽取的采样体验，允许极其有效地开发特性。

### UNREAL AGENT

该算法结合了两种独立的、最先进的方法的优点，用于深度强化学习。主要策略由A3C训练。它从并行的经验流中学习，以获得效率和稳定性；使用策略评估方法在线更新；它使用递归神经网络来编码完整的经验历史。这允许代理在部分观察到的环境中有效地学习。

辅助任务是根据最近存储和随机采样的经验序列进行训练的; 这些序列可以优先考虑（在我们的例子中，根据立即奖励排序）; 通过Q学习对这些目标进行off-policy训练; 他们可能会使用更简单的前端架构。 这允许以最高效率训练表示。

UNREAL算法针对代理的联合参数 $$θ$$ 优化单个组合损失函数，其将A3C损失 $$L_{A3C}$$ 与辅助控制损失 $$L_{PC}$$ ，辅助奖励预测损失 $$L_{RP}$$ 和重放价值损失 $$L_{VR}$$ 组合

$$\mathcal{L}_{U N R E A L}(\theta)=\mathcal{L}_{\mathrm{A} 3 \mathrm{C}}+\lambda_{\mathrm{VR}} \mathcal{L}_{\mathrm{VR}}+\lambda_{\mathrm{PC}} \sum \mathcal{L}_{Q}^{(c)}+\lambda_{\mathrm{RP}} \mathcal{L}_{\mathrm{RP}}$$ 

## 实验

迷宫任务和雅达利游戏机

![](../../.gitbook/assets/image%20%2845%29.png)

