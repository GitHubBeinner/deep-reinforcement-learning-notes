# Deep GA

## 介绍

> [Deep Neuroevolution: Genetic Algorithms are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning](https://arxiv.org/pdf/1712.06567.pdf)

深度人工神经网络（DNN）通过基于梯度的学习算法（即反向传播）进行常规训练。 进化策略（ES）可以与基于反向基础的算法（如Q-learning和策略梯度）相媲美，以解决深层强化学习（RL）问题。 然而，ES可以被认为是基于梯度的算法，因为它通过类似于梯度的有限差分近似的操作来形成随机梯度下降。 这提出了一个问题，即基于非梯度的进化算法是否可以在DNN尺度上发挥作用。在这里我们可以说明：我们使用简单的，无梯度，基于种群的遗传算法（GA）来演化DNN的权重，并且它可以解决硬的深RL问题，包括Atariand人形运动。 Deep GA成功地发展了具有超过四百万个自由参数的网络，这是最大的神经网络随着传统的进化算法而发展的。这些结果（1）扩展了我们对GA可以运作的规模的认识，（2）有趣的是，在某些情况下，跟随梯度不是优化性能的最佳选择，（3）立即可用于多种神经进化技术改进性能。我们通过展示将DNN与新颖性搜索结合，这种模式鼓励在具有欺骗性或稀疏回报函数的任务上进行探索，可以解决回报最大化算法\(例如DQN、A3C、ES和遗传算法\)的失败。此外，深度GA比ES、A3C和DQN更快\(在一个桌面上训练Atari 4小时，或者在720个内核上训练Atari 1小时\)，并且支持最先进的、高达10，000倍的紧凑编码技术。

## 算法

本文使用的遗传算法是非梯度的，仅使用突变和筛选获取更强的个体，这和前面的ES算法是截然不同的。

### 遗传算法

![](../../.gitbook/assets/image-40.png)

### 新颖搜索

![](../../.gitbook/assets/image-24.png)

### 为什么GA比ES快？

由于以下两个主要原因，GA比ES更快：（1）对于每次生成，ES必须计算如何更新其神经网络参数向量 $$θ$$ 。 它是通过加权平均值来实现的，这些平均值是通过它们的大量伪后代（随机 $$θ$$ 扰动）的加权平均值。 这种平均操作对于大型神经网络和大量伪后代（后者需要健康的优化）来说是缓慢的，并且对于Deep GA来说不是必需的。 （2）ES要求虚拟批量规范化以在伪后代之间生成不同的策略，这对于准确的有限差分近似是必需的。 虚拟批次规范化需要参考批次的额外前向传递 - 在训练开始时选择的随机观察集 - 计算层规范化统计数据，然后以与批量标准化相同的方式使用。 我们发现随机GA参数扰动产生了足够多样的策略而没有虚拟批量归一化，从而避免了这些额外的前向传递通过网络。

### 群体编码方法

![](../../.gitbook/assets/image-44.png)

## 实验

![](../../.gitbook/assets/image-91.png)

