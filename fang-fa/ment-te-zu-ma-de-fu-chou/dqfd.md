# DQfD

## 介绍

> [Deep Q-learning from Demonstrations](https://arxiv.org/pdf/1704.03732.pdf)

深度强化学习在困难的决策问题上取得了一些引人注目的成功。然而，这些算法在达到合理的性能之前通常需要大量的数据。事实上，他们在学习中的表现可能非常差。这对于模拟器来说可能是可以接受的，但是它严重限制了深度学习在许多现实世界任务中的应用，其中代理必须在真实环境中学习。在本文中，我们研究了代理可以从系统的先前控制中访问数据的设置。我们提出了一种算法，深度Q -从演示中学习\( DQfD \)，它利用少量演示数据来大规模加速学习过程，即使是从相对少量的演示数据中，也能够在学习的同时自动评估演示数据的必要比率，这得益于优先级重放机制。DQfD的工作原理是将时间差异更新与守护者行为的监督分类相结合。 我们证明DQfD具有比优先决定双深度Q网络（PDD DQN）更好的初始性能，因为它在42场比赛中的41场比赛中获得了更高的分数，并且平均需要PDD DQN8300万步才能赶上 DQfD的表现。此外，DQfD利用人类的表现来实现11场比赛的最新成绩。 最后，我们证明了DQfD比三个相关的算法表现更好，可以将演示数据合并到DQN中。

监督损失使算法能够学习模仿演示者，而TD损失使算法能够学习自洽的值函数，从而能够继续学习RL。经过预训练后，代理开始用其学习到的策略与域交互。代理通过演示和自生成数据的混合来更新网络。实际上，在学习过程中选择演示数据和自生成数据的比例对于提高算法的性能至关重要。我们的一个贡献是使用优先重播机制来自动控制这种比率。

## 方法

监督损失对于预训练产生任何效果至关重要。由于演示数据必然覆盖了状态空间的一个狭窄部分，并且没有采取所有可能的动作，因此从未采取过许多状态动作，也没有数据来使它们符合实际值。如果我们对网络进行预训练，只对下一个状态的最大值进行问学习更新，网络将向这些无根据的变量中的最高值更新，并且网络将通过问函数传播这些值。我们增加了large margin classification loss：

$$
J_{E}(Q)=\max _{a \in A}\left[Q(s, a)+l\left(a_{E}, a\right)\right]-Q\left(s, a_{E}\right)
$$

其中 $$a_{E}$$ 是演示数据选择的动作， $$l\left(a_{E}, a\right)$$ 是margin function（相等时为0，其它为正），这种损失迫使其他行为的价值至少比演示者行为的价值低一个限度。将这种损失加在一起，使得看不见的行为的价值成为合理的价值，并使价值函数所诱导的贪婪政策模仿决策者。如果算法仅用这种普遍的损失进行预训练，则不会有任何约束连续状态之间的值，并且Q网络不会满足贝尔曼方程，而贝尔曼方程是通过TD学习在线改进策略所必需的。

加上 n-step return

$$
r_{t}+\gamma \tau_{t+1}+\ldots+\gamma^{n-1} r_{t+n-1}+\max _{a} \gamma^{n} Q\left(s_{t+n}, a\right)
$$

再加上L2正则化损失得到联合损失函数

$$J(Q)=J_{D Q}(Q)+\lambda_{1} J_{n}(Q)+\lambda_{2} J_{E}(Q)+\lambda_{3} J_{L 2}(Q)$$

一旦预训练阶段完成，代理就开始在系统上进行操作，收集自己生成的数据，并将其添加到重放缓冲区中。数据被添加到播放缓冲区，直到它满了，然后代理开始在该缓冲区中重写旧数据。然而，代理从不重写演示数据。对于比例优先采样，不同的小的正常量 $$\epsilon_{a}$$和$$\epsilon_{d}$$  被添加到代理和演示转换的优先级中，以控制演示与代理数据的相对采样。所有损失都应用于两个阶段的演示数据，而监督损失不应用于自行生成的数据。

### 伪代码

![](../../.gitbook/assets/image%20%2862%29.png)

## 实验

![](../../.gitbook/assets/image%20%2895%29.png)

![](../../.gitbook/assets/image%20%28111%29.png)

![](../../.gitbook/assets/image%20%28157%29.png)





