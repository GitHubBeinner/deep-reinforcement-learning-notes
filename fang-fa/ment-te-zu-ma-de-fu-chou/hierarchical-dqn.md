# Hierarchical-DQN

## 介绍

> [Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation](https://arxiv.org/abs/1604.06057)

稀疏反馈环境下的学习目标导向行为是增强学习算法面临的主要挑战。主要的困难是由于探索不够，导致代理无法学习健壮的值函数。具有内在动机的代理可以为了自身的利益而探索新的行为，而不是直接解决问题。这样的内在行为最终可以帮助代理解决环境设置的任务。提出了一种基于层次结构的dqn \(h-DQN\)框架，该框架集成了不同时间尺度下的层次值函数，具有内在的深层强化学习动机。高层值函数学习策略而不是内在目标，底层函数学习满足给定目标的原子操作。h-DQN允许灵活的目标规范，例如实体和关系上的函数。这为复杂环境中的探索提供了有效的空间。我们证明了我们的方法在两个非常稀疏、延迟反馈的问题上的优势:\(1\)一个复杂的离散随机决策过程，和\(2\)经典的ATARI游戏Montezuma的复仇。

## 算法



