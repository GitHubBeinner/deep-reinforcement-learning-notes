# Beating Atari with Natural Language Guided Reinforcement Learning

## 介绍

> [Beating Atari with Natural Language Guided Reinforcement Learning](https://arxiv.org/pdf/1704.05539.pdf)

我们介绍了第一个借助自然语言指导学会击败Atari游戏的深层强化学习代理。 代理人使用环境观察和自然语言之间的多模态嵌入来通过英语指令列表自我监控进度，除了增加游戏分数之外，还为补充指令提供奖励。 我们的代理显着优于Deep Q-Networks（DQN），异步优势演员 - 评论家（A3C）代理，以及最佳代理发布到OpenAI Gym \[4\]，其中被认为是最困难的Atari 2600环境\[2\]：MONTEZUMA' SREVENGE。

![](../../.gitbook/assets/image%20%2817%29.png)

人类通常不会在真空中学会与世界互动，缺乏与他人的互动，我们也不会生活在无国籍、单一范例的监督学习世界中。

相反，我们生活在一个奇妙复杂和有状态的世界中，过去的行为会影响当前的结果。 在我们的学习中，我们受益于他人的指导，接受自然语言中任意高级别的指导 - 并学习填补这些指示之间的空白 - 作为一个具有不同奖励来源的内在和外在的世界。

受到以自然语言的力量指导人工智能的梦想的启发，我们开始创建一个能够学习高级英语教学的代理，因为它学会了在Atari游戏的模型世界中行动。

我们通过将自然语言处理和深度强化学习的技术结合在两个阶段来实现:第一阶段，代理学习英语命令的含义，以及它们如何映射到游戏状态的观察上。在第二阶段，代理探索环境，通过它已经学会的命令来理解和学习满足给定命令需要什么动作。

## 方法

![](../../.gitbook/assets/image%20%287%29.png)

### Viability of Sub-Task Rewards

![](../../.gitbook/assets/image%20%2890%29.png)

有了这个更有希望的基线，并了解了我们对理论状态的理解，我们开始运行一个关于额外指导对强化学习者有用的概念证明，只要在我们确定有益于学习者学习的状态下，就向学习者注射额外的奖励信号。我们并没有基于完成自然语言指令而获得额外的奖励，我们最初只是简单地奖励到达游戏状态的代理，即球的位置直接位于球拍的位置之上。为了实现这一点，我们为Breakout创建了一个模板匹配库，以返回球和桨在环境中的位置。事实证明，这很重要，因为突破环境中的桨随着游戏的进行而变小，在某些场景中会与墙壁和球重叠，球会根据y位置改变颜色，这只是一些复杂的情况。

有了这个额外的模板匹配奖励，代理人获得了击球状态，我们看到游戏早期阶段的学习率有了显著的提高。当代理人失去生命时，我们也看到了更多的改进，也提供了负面的回报（这种奖励信号不是由环境直接提供的）。 在我们的两条硬编码指令之后的额外奖励信号帮助代理在仅12,000次迭代（每次迭代播放128帧）后得分超过120分，比相同训练时间后的普通A3C好三倍多。

虽然这些早期训练的结果是有希望的，但是随着代理人在环境中的进一步发展，额外的奖励信号变得不那么重要。在36，000次迭代之后，额外的奖励代理与基线A3C Breakout模型收敛，并且在100，000次迭代之后，它实际上性能更差。我们将这归因于以下两个因素：1）当球比球桨更快地移动时，简单地将球保持在球下方的事实对于后续轮次的策略不是有用的，以及2）基线A3C代理学习的事实 没有额外的奖励信号如此迅速，因为在Breakout环境中奖励是密集的。

### Experiments in Multimodal Embeddings

在对增强（A3C）和奖励增强子组件进行实验和调试之后，我们尚未验证的模型的最后一个组成部分是多模式嵌入将自然语言描述和帧映射到单个嵌入空间，我们可以确定描述是否适用于帧。

我们首先通过运行我们刚刚为Breakout训练的A3C模型生成了几千帧的数据集，并使用我们的模板匹配代码来给出帧中实体的关系描述。 例如，一些可能的框架描述包括“球位于球拍的右侧”和“球拍位于球的右侧”。

为了正确地识别一系列连续帧所满足的命令，并将描述这些命令和帧的固定大小向量传递给我们的学习代理，我们需要在帧和句子之间建立多模态模型。整个设置如图4的绿色部分所示。我们的网络把一对连续的帧和一个句子作为一个词的向量，这个向量可以描述也可以不描述图像。帧通过卷积神经网络进行嵌入。

![](../../.gitbook/assets/image%20%2878%29.png)

该网络旨在最大化生成的框架嵌入和准确描述一系列框架的所有句子的句子嵌入之间的点积，并最小化不适用于框架的句子的句子嵌入的点积。我们选择框架和句子嵌入之间的点积，例如余弦距离，因为许多句子可能匹配一个框架。

### MONTEZUMA’SREVENGE

我们为该游戏编写了一个新的模板匹配库，并制定了一系列新的命令，这些命令不仅对代理逃离第一个房间有用，而且可以在该层的不同房间中推广。

就像 Breakout一样，我们从一个更简单的实验开始，而不是直接使用这些自然语言命令，我们首先奖励代理，以便简单地访问图1所示房间的不同手动指定位置。通过这种方法，我们在用尽我们的指令之前，能够达到400分的平均分数，并且一致地让代理离开第一个房间。这些结果对我们最终的目标是有希望的，因为它们证实了一些信息辅助奖励的来源有助于代理到达更远的地方。

### Dataset Generation

已经证明子任务奖励非常适合强化学习代理学习玩MONTEZUMA'SREVENGE，我们接下来的任务是生成一个数据集，以学习状态的自然语言描述与环境中的原始像素之间的映射。然而，没有这样的数据集存在，所以我们创建了我们自己的游戏状态到他们满意的自然语言描述的映射。为了做到这一点，我们在整个游戏中玩了几次，每次都保存了游戏状态的帧。利用模板匹配代码，我们可以生成一系列给定的连续帧所满足的选定命令列表。

### Learning Frame-Command Mappings with Bimodal Network

与Breakout一样，训练的第一阶段构成了在帧对之间创建多模式嵌入，描绘了附录A中的玩家运动和命令语句。嵌入训练使得当帧满足命令时，帧与命令嵌入之间存在正点积。当命令不满足时，点产品被训练为否定的。我们使用具有字级嵌入的LSTM来提取命令嵌入，并且使用卷积神经网络在堆叠在信道维度上的成对帧上运行来进行帧嵌入。

#### Evidence of Generalization

考虑到双峰嵌入模型的复杂性及其对包含第一级所有房间的训练数据集的访问，我们想证明双峰实际上是在学习如何理解命令的含义，而不仅仅是要求代理处于与训练数据中完全相同的位置来将命令分类为完整的。

![](../../.gitbook/assets/image%20%28143%29.png)

嵌入方案似乎保持其极低的错误率，即使在训练期间没有观察到的房间测试时也是如此。 这提供了一些证据表明嵌入在一定程度上概括了整个房间，而不仅仅是房间内的行为。

### Run-Time Learning From Natural Language Reward Using Multimodal Embedding

随着多模态嵌入训练的完成，我们进入强化学习阶段。对于每个A3C worker，我们加载嵌入权重和代理列表以顺序完成。我们通过使用我们的预训练双峰嵌入来计算命令是否已完成，通过当前观察状态和当前命令来完成网络，并且如果结果帧和命令嵌入之间的点积为正，则将命令标记为已完成。如果我们注意到命令已经完成，我们会为我们的执行学习代理提供额外的奖励，以便成功完成任务并继续执行下一个命令，同时将嵌入作为附加功能提供给学习代理，如图4所示。

## 最终结果

![](../../.gitbook/assets/image%20%28130%29.png)





