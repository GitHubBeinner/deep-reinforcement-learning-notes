# LSTM-DQN

> [Language understanding for textbased games using deep reinforcement learning](https://arxiv.org/abs/1506.08941)

在本文中，我们考虑学习基于文本的游戏的控制策略的任务。在这些游戏中，虚拟世界中的所有互动都是通过文本进行的，不理解的状态是不会被观察到的。不断产生的语言障碍使得这样的环境对自动游戏玩家来说具有挑战性。我们采用深度强化学习框架，通过游戏奖励作为反馈，共同学习状态表征和行动策略。这种框架使我们能够将文本描述映射到矢量表示中，从而捕捉游戏状态的这些特征。我们在两个游戏世界中评估我们的方法，用单词袋和二元袋作为状态代表与基线进行比较。我们的算法在两个世界上都优于基线，证明了学习表达的重要性。

![](../../.gitbook/assets/image%20%28163%29.png)

## 方法

在这一部分，我们描述了我们的模型\(DQN\)，并描述了它在学习随机文本描述的博弈的好的Q值方法中的应用。我们把模型分成两部分。第一个模块是表示生成器，用于将当前状态的文本描述转换为向量。这个向量然后被输入到第二个模块，它是一个计分器。图2显示了我们模型的整体架构。我们使用游戏中的奖励反馈，共同学习表示生成器和动作评分器的参数

![](../../.gitbook/assets/image%20%28179%29.png)

玩MUD游戏的另一个复杂性是玩家采取的动作是多词自然语言命令，例如eat apple 和 go east。由于计算上的限制，在这项工作中，我们将自己限制在考虑由一个动作\(例如eat\)和一个参数对象\(例如苹果\)组成的命令。对于我们世界中的大多数命令，这种假设是有效的，除了一类需要两个参数的命令（例如，movered-root right，move blue-root up）。我们考虑游戏中可用的所有可能的操作和对象，并使用相同的网络预测每个状态（图2）。我们认为整个命令\(a，o\)的Q值是动作a和对象o的Q值的平均值。

算法描述如下

![](../../.gitbook/assets/image%20%28183%29.png)

