# 监督学习

在监督学习中，通过实例训练模型。在培训过程中，要求模型做出正确答案已知的决策。误差，即所提供的答案与预测值之间的差异，被用来作为损失来更新模型。我们的目标是建立一个超越训练数据的模型，从而在它从未见过的例子上表现出色。大数据集通常可以提高模型的泛化能力。

在游戏中，这些数据可以来自游戏轨迹（即人们在玩游戏的记录），允许代理基于人类在动作状态下执行的动作来学习从输入状态到输出动作的映射。 如果游戏已经通过另一种算法解决，它可以用于生成训练数据，如果第一种算法太慢而无法实时运行，这将非常有用。 虽然学习从现有数据中学习可以让代理人快速学习最佳实践，但它往往很脆弱; 可用的数据可能很昂贵，并且可能缺少代理应该能够处理的关键方案。 对于游戏玩法，该算法仅限于数据中可用的策略，并且不能探索新的内容。 因此，在游戏中，监督算法通过强化学习算法与额外的训练相结合。

游戏中监督学习的另一个应用是学习游戏的状态转换。神经网络可以学习预测动作状态对的下一个状态，而不是提供给定状态的动作。因此，网络本质上是在学习一个游戏模型，然后可以用来更好地玩游戏或进行规划

# 无监督学习

非监督学习的目的不是学习数据与其标签之间的映射，而是发现数据中的模式。这些算法可以了解数据集的特征分布，这些特征可以用来聚类相似的数据，将数据压缩成其基本特征，或者创建新的合成数据，这是原始数据的特征。对于奖励稀少的游戏\(如蒙特祖玛的复仇\)，以无监督的方式从数据中学习是一个潜在的解决方案，也是一个重要的开放式深度学习挑战

深度学习中一种突出的无监督学习技术是自动编码器，它是一种神经网络，试图学习恒等函数，使得输出与输入相同。 该网络由两部分组成：一个将输入x映射到低维隐藏向量h的编码器，以及一个试图从x重构的解码器。主要思想是通过保持h很小，网络通过压缩数据，从而学会了一个好的 表示。 研究人员开始将这种无监督算法应用于游戏，以帮助提取有意义的低维数据的高维数据，但这种研究方向仍处于早期阶段。

# 强化学习

![](/assets/rl.png)



在强化学习（RL）中，代理通过与向代理提供奖励信号的环境交互来学习行为。 视频游戏可以很容易地被建模为RL设置中的环境，其中玩家被建模为具有可以在每个步骤采取的有限动作集的代理人，并且奖励信号可以由游戏分数确定。

在RL中，代理依赖于奖励信号。 这些信号可以经常出现，例如游戏中的得分变化，也可以很少出现，例如直到代理人赢或输了游戏才出现。 电子游戏和RL可以很好地结合在一起，因为大多数游戏都会为成功的策略带来回报。开发世界游戏通常并没有明确的奖励模型，所以对RL算法具有挑战性。

